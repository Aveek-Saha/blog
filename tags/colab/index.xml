<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>colab on Aveek's Blog</title><link>https://home.aveek.io/blog/tags/colab/</link><description>Recent content in colab on Aveek's Blog</description><generator>Hugo -- gohugo.io</generator><managingEditor>aveek.s98@gmail.com (Aveek Saha)</managingEditor><webMaster>aveek.s98@gmail.com (Aveek Saha)</webMaster><lastBuildDate>Mon, 24 Jun 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://home.aveek.io/blog/tags/colab/index.xml" rel="self" type="application/rss+xml"/><item><title>Saving a Keras model to persistent storage</title><link>https://home.aveek.io/blog/post/checkpointing-keras-model/</link><pubDate>Mon, 24 Jun 2019 00:00:00 +0000</pubDate><author>aveek.s98@gmail.com (Aveek Saha)</author><guid>https://home.aveek.io/blog/post/checkpointing-keras-model/</guid><description>
&lt;p>Learn how to save Keras models to persistent storage or your Google drive and resume training it from where you left off.&lt;/p>
&lt;p>Learn how to save Keras models to persistent storage or your Google drive and resume training it from where you left off.&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>A lot of the time deep learning models can take several hours, days or weeks to train and if the machine that it&amp;rsquo;s running on shuts down unexpectedly before training is finished, it can lead to all that work going to waste. This introduces the need for a way to save and load models so that training can be continued from a certain checkpoint.&lt;/p>
&lt;p>Luckily Keras has got us covered with the &lt;a href="https://keras.io/callbacks/#modelcheckpoint">&lt;code>ModelCheckpoint&lt;/code>&lt;/a> callback class.&lt;/p>
&lt;h1 id="saving-a-model">Saving a model&lt;/h1>
&lt;p>Lets say you have a simple neural network&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">keras&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Sequential&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">keras.layers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Dense&lt;/span>
&lt;span class="n">classifier&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Sequential&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Dense&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;relu&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">input_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">56&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Dense&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;relu&amp;#39;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Dense&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;sigmoid&amp;#39;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">compile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;adam&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">loss&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;binary_crossentropy&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">metrics&lt;/span> &lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;accuracy&amp;#39;&lt;/span>&lt;span class="p">])&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>Before fitting the model, create a ModelCheckpoint object, we&amp;rsquo;ll go over what each of the parameters do in a minute, but for now, in this example after each epoch the model will be saved to a &lt;code>hdf5&lt;/code> file in the current working directory.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">keras.callbacks&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">ModelCheckpoint&lt;/span>
&lt;span class="n">filepath&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;./weights-{epoch:02d}-{val_acc:.3f}.hdf5&amp;#34;&lt;/span>
&lt;span class="n">checkpoint&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ModelCheckpoint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filepath&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">monitor&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;val_acc&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">verbose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mode&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;max&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">callbacks_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">checkpoint&lt;/span>&lt;span class="p">]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>Now pass the callback list while fitting the model&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">epochs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">20&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">validation_data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_validation&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Y_validation&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks_list&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>Once the model finishes running, you&amp;rsquo;ll be able to see 20 &lt;code>hdf5&lt;/code> files labeled by their epoch number and validation accuracy in your working directory.&lt;/p>
&lt;h1 id="parameters">Parameters&lt;/h1>
&lt;p>There are several parameters that can be passed to ModelCheckpoint, we&amp;rsquo;ll go over them here&lt;/p>
&lt;ul>
&lt;li>&lt;strong>filepath&lt;/strong>: a string, the path where you want to save the model file.&lt;/li>
&lt;li>&lt;strong>monitor&lt;/strong>: quantity to monitor. Eg: val_acc, acc, val_loss, loss, etc.&lt;/li>
&lt;li>&lt;strong>verbose&lt;/strong>: verbosity mode, 0 or 1.&lt;/li>
&lt;li>&lt;strong>save_best_only&lt;/strong>: if it&amp;rsquo;s True, then the model will only be saved if the new model improves over the last saved model in the monitored quantity.&lt;/li>
&lt;li>&lt;strong>save_weights_only&lt;/strong>: if True, then only the model&amp;rsquo;s weights will be saved, else the full model is saved (including the optimizer state).&lt;/li>
&lt;li>&lt;strong>mode&lt;/strong>: one of {auto, min, max}. If =True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity.&lt;/li>
&lt;li>&lt;strong>period&lt;/strong>: number of epochs between checkpoints.&lt;/li>
&lt;/ul>
&lt;h1 id="save-model-to-google-drive-on-colab-notebooks">Save model to Google Drive on Colab notebooks&lt;/h1>
&lt;p>Google&amp;rsquo;s Colab notebooks are a great way to start prototyping and creating neural networks and machine learning models. If you choose a GPU runtime, you are given a free GPU that can greatly reduce your model training time.&lt;/p>
&lt;p>As great is Colab is, it does have some caveats&lt;/p>
&lt;ol>
&lt;li>If you are disconnected from the internet or close the window for more than 90 minutes, the runtime automatically shuts down or gets recycled.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>You are given a maximum of 12 hours at a time on the VM instance.&lt;/li>
&lt;/ul>
&lt;p>In both these cases you may be able to reconnect to the instance but all your local variables will be lost.&lt;/p>
&lt;p>Saving the model to Google Drive after every epoch makes it easy to just restart training from the last epoch that was saved.&lt;/p>
&lt;p>Google makes it super easy to do this on Colab. First we have to allow Colab to access Google Drive.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">google.colab&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">drive&lt;/span>
&lt;span class="n">drive&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mount&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/content/gdrive&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>When you run this, it generates a link, click on it. Select the google account whose drive you want to mount. Then it takes you to a new tab that says &lt;code>Google Drive File Stream wants to access your Google Account&lt;/code>. On clicking &lt;code>allow&lt;/code> it generates an authorization code that you have to paste in a text box that appears bellow the code you just ran. Paste the code and hit &lt;code>enter&lt;/code>.&lt;/p>
&lt;p>On refreshing your file browser, you should see a folder called &lt;code>gdrive&lt;/code>, that&amp;rsquo;s the mounted drive folder.&lt;/p>
&lt;p>To save it to your drive, the code is almost the same as the example in the previous section, but the filename should be changed, so that the model is stored in drive.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="c1"># Create a folder in your drive called model before running this&lt;/span>
&lt;span class="n">filepath&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/content/gdrive/My Drive/model/weights-{epoch:02d}-{val_acc:.3f}.hdf5&amp;#34;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;h1 id="resume-training-from-a-saved-model">Resume training from a saved model&lt;/h1>
&lt;p>If you saved the entire model (not just the weights) then the model can continue training from wherever it stopped.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">keras.models&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">load_model&lt;/span>
&lt;span class="n">classifier&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">load_model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;/content/gdrive/My Drive/model/weights-15-0.815.hdf5&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">classifier&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">epochs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">20&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">validation_data&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_validation&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Y_validation&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">callbacks&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">callbacks_list&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">initial_epoch&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></description></item></channel></rss>